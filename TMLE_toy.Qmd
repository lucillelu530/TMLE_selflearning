# TMLE/Superlearner Self-learning Note

Reference: https://www.khstats.com/blog/tmle/tutorial-pt2

`superlearner` tutorial: Â <https://tlverse.org/tlverse-handbook/sl3.html#defining-learners-over-grid-of-tuning-parameters>

## Application Example (just a simple one!)

### Initial setup

-   $Y:$ Binary outcome
-   $A:$ Binary Treatment
-   $W:$ Covariates (four variables, half discrete and half continuous)

```{r}
library(tidyverse) # for data manipulation
library(gt) # for table printing
library(SuperLearner) # for ensemble learning

set.seed(7) # for reproducible results

## Simulate dataset
generate_data <- function(n){ 
    W1 <- rbinom(n, size=1, prob=0.2) # binary confounder
    W2 <- rbinom(n, size=1, prob=0.5) # binary confounder
    W3 <- round(runif(n, min=2, max=7)) # continuous confounder
    W4 <- round(runif(n, min=0, max=4)) # continuous confounder
    A  <- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders
    Y <- rbinom(n, size=1, prob= plogis(-1 + A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4))) # binary outcome depends on confounders
    return(tibble(Y, W1, W2, W3, W4, A))
}

n <- 1000
dat_obs <- generate_data(n) # generate a data set with n observations

dat_obs %>%
  head() %>%
  gt() %>%
  tab_header("Simulated data set.")


```

### Targeted Estimand: ATE

$$
ATE=\Phi=E_W[E[Y|A=1,W]-E[Y|A=0,W]]
$$

Before estimating, pick up some base machine learning algorithms via `SuperLearner` to estimate the expected outcome (Q function) and probability of treatment.

```{r}
sl_libs <- c('SL.glmnet', 'SL.ranger', 'SL.earth', 'SL.glm') 
```

### Step1: **Estimate the Outcome (Q-function)**

Review: Q function is the condition expectation of $Y$ when conditioned on A and W.

$$
Q(A,W)=E(Y|A,W)
$$

In this step use multiple machine learning methods to estimate the Q function is best accepted since it does not require underlying assumption of data structure.

```{r}
Y <- dat_obs$Y
W_A <- dat_obs %>% select(-Y) # remove the outcome to make a matrix of predictors (A, W1, W2, W3, W4) for SuperLearner
Q <- SuperLearner(Y = Y,
                  X = W_A, 
                  family=binomial(), 
                  SL.library = sl_libs) # specify our superlearner library of LASSO, RF, and MARS
```

```{r}
Q_A <- as.vector(predict(Q)$pred) # obtain predictions for everyone using the treatment they actually received
W_A1 <- W_A %>% mutate(A = 1)  # data set where everyone received treatment
Q_1 <- as.vector(predict(Q, newdata = W_A1)$pred) # predict on that everyone-exposed data set
W_A0 <- W_A %>% mutate(A = 0) # data set where no one received treatment
Q_0 <- as.vector(predict(Q, newdata = W_A0)$pred)
```

Through the above progress, we can obtain three items:

$$
\begin{aligned}
\hat{Q}(A,\mathbf{W})&=\hat{E}(Y|A,W) \\
\hat{Q}(1,\mathbf{W})&=\hat{E}(Y|A=1,W)\\
\hat{Q}(0,\mathbf{W})&=\hat{E}(Y|A=0,W)
\end{aligned}
$$

(1): When every individual received treatment they actually received

(2): If every individual received the treatment ($A=1$)

(3): If every individual received the control ($A=0$)

```{r}
dat_tmle <- tibble(Y = dat_obs$Y, A = dat_obs$A, Q_A, Q_0, Q_1)
dat_tmle %>%
  head() %>%
  gt() %>%
  tab_header("TMLE Algorithm after Step 1")
```

#### If No updating, use G-formula

-   **standardization**, **simple substitution estimation**, **g-formula estimation**, or **G-computation**

-   $$
    \hat{ATE}_{G-comp}=\frac{1}{N}\sum_{i=1}^N(\hat{E}(Y|A=1,W)-\hat{E}(Y|A=0,W))
    $$

```{r}
ate_gcomp <- mean(dat_tmle$Q_1 - dat_tmle$Q_0)
cat("The G-computation for ATE :", ate_gcomp,"\n")
```

-   Flaws of G-formula: It does not have appropriate trade-off variance for ATE, because G-formula was built to have the best bias-variance tradeoff for estimating the *outcome*, conditional on confounders, rather than the ATE.

    -   Cannot compute the SD of the estimator because the sampling distribution of machine learning estimates

### Step 2: Estimate the probability of Treatment (Propensity score)

$$
g(\mathbf{W})=P(A=1|\mathbf{W})
$$

How to estimate the propensity score: same as Q-function (i.e. using superlearner)

```{r}
A <- dat_obs$A
W <- dat_obs %>% select(-Y, -A) # matrix of predictors that only contains the confounders W1, W2, W3, and W4
g <- SuperLearner(Y = A, # outcome is the A (treatment) vector
                  X = W, # W is a matrix of predictors
                  family=binomial(), # treatment is a binomial outcome
                  SL.library=sl_libs) # using same candidate learners; could use different learners
```

#### Inverse Probability of receiving treatment

$$
H(1,\mathbf{W})=\frac{1}{g(\mathbf{W})}=\frac{1}{Pr(A=1|\mathbf{W})}
$$

```{r}
g_w <- as.vector(predict(g)$pred) # Pr(A=1|W)
H_1 <- 1/g_w
```

#### Negative Inverse Probability of not receiving treatment

$H(0,\mathbf{W})=-\frac{1}{P(A=0|\mathbf{W})}$

```{r}
H_0 <- -1/(1-g_w) # Pr(A=0|W) is 1-Pr(A=1|W)
```

#### Calculate the Clever Covariate

$$
H(A,W)=\frac{I(A=1)}{g(\mathbf{W})}-\frac{I(A=0)}{1-g(\mathbf{W})}
$$

```{r}
dat_tmle <- # add clever covariate data to dat_tmle
  dat_tmle %>%
  bind_cols(
         H_1 = H_1,
         H_0 = H_0) %>%
  mutate(H_A = case_when(A == 1 ~ H_1, # if A is 1 (treated), assign H_1
                       A == 0 ~ H_0))  # if A is 0 (not treated), assign H_0
```

### Step 3: Estimate the Fluctuation Parameter

Idea: machine learning fits have an optimal bias-variance trade-off for estimating the outcome (conditional on treatment and confounders), rather than the ATE(the mean of difference)

-   You might review the concepts regarding semiparametric theory:eg. EIF (efficient influence function) and EQ (Estimating equation)

-   **Use the information from step2(i.e. treatment mechanism) to optimize the bias-variance trade-off for** $ATE$

**Estimating equation**

$$
logit(E[Y|A,\mathbf{W}])=logit(\hat{E}[Y|A,\mathbf{W}])+\epsilon H(A,\mathbf{W})
$$

-   Left side: True outcome of $Y$ on the logit scale

-   Right side: Indicate that the left side can be seen as a logistic regression ($logit(E[Y|A,\mathbf{W}]=\beta_0+\beta_1X$), the key is: $logit(\hat{E}[Y|A,\mathbf{W}]$ is not constant, it is a vector of values. We can see it as an **offset** or a **fixed intercept** in a logistic regression, rather than a constant-value intercept.

    -   My question: difference between intercept and offset

    -   $logit(\hat{E}[Y|A,\mathbf{W}]$: Initial outcome estimate on the logit scale (obtain from `Step 1`

    -   Fit the logistic regression = update outcome Y (estimate the fluctuation parameter $\epsilon$ )

```{r}
glm_fit <- glm(Y ~ -1 + offset(qlogis(Q_A)) + H_A, data=dat_tmle, family=binomial)
#(a) use qlogis to transform the probabilities to transform the E(Y|A,W), the probability, to logit scale
#(b) fixed intercept: -1+offset(logit(Q_hat))
eps <- coef(glm_fit) #fluctuation parameter
```

#### About **fluctuation parameter**

it provides information about how much to change, or fluctuate, our initial outcome estimates.

### Step 4: Update the **Initial Estimates of the Expected Outcome**

Key Idea: **update our initial outcome estimates** from Step 1 using information from Step 2 and 3 to obtain the correct bias-variance tradeoff for the ATE

-   Intuition & Review: We first initialize the estimate of outcome (Q function) and want to update it.

    -   To update, we first define a `clever coveriate` (Step 2, use inverse probability as the weighting), and use such item as the information of treatment mechanism

    -   In step 3, we then estimate the EIF (efficient influence function), and the fluctuation parameter. What we obtained in the previous steps are essential update (i.e. step 4)

**Step 3**: The estimated EIF and $\epsilon$ are in the logit scale.

So first do `logit` transform for the initialized expected outcome

Do Update:

$$
logit(\hat{E}^*(Y|A,\mathbf{W}))=logit(\hat{E}(Y|A,\mathbf{W}))+\hat{\epsilon}H(A,\mathbf{W})
$$

Then transform back:

$$
\hat{E}^*(Y|A,\mathbf{W})=expit(logit(\hat{E}(Y|A,\mathbf{W}))+\hat{\epsilon}H(A,\mathbf{W}))
$$

**Update the expected outcomes of all observations, given the treatment they actually received and their baseline confounders.**

```{r}
H_A <- dat_tmle$H_A # for cleaner code in Q_A_update
Q_A_update <- plogis(qlogis(Q_A) + eps*H_A)

Q_1_update <- plogis(qlogis(Q_1) + eps*H_1) #Update the expected outcomes, conditional on baseline confounders and everyone receiving the treatment. E(Y|A=1,W)

Q_0_update <- plogis(qlogis(Q_0) + eps*H_0)#Update the expected outcomes, conditional on baseline confounders and no one receiving the treatment. E(Y|A=0,W)
```

### Step 5: Compute the estimate of ATE

```{r}
tmle_ate <- mean(Q_1_update - Q_0_update)
tmle_ate
```

## Using `tmle` Package instead

```{r}
library(sl3)
tmle_fit <-
  tmle::tmle(Y = Y, # outcome vector
           A = A, # treatment vector
           W = W, # matrix of confounders W1, W2, W3, W4
           Q.SL.library = sl_libs, # superlearning libraries from earlier for outcome regression Q(A,W)
           g.SL.library = sl_libs) # superlearning libraries from earlier for treatment regression g(W)

tmle_fit


```
